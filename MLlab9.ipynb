{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "def load_data(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  df.head()\n",
    "  Y = df['5'].to_numpy()\n",
    "  del df['5']\n",
    "  X=df.to_numpy()\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(\"mnist_train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=42)\n",
    "#One hot encoding of training labels \n",
    "Labels=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "class Perceptron():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            x=self.softmax(z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            prev_term=self.delta_mll(y,self.y_pred)  \n",
    "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            x=self.softmax(z) \n",
    "        return np.argmax(x,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0012043920645853539\n",
      "1 0.001450473151265108\n",
      "2 0.0015677496484550737\n",
      "3 0.0014187256672389906\n",
      "4 0.0009610943879825327\n",
      "5 0.0008556319485332141\n",
      "6 0.0007689899926799949\n",
      "7 0.000642238944402946\n",
      "8 0.0005579156300243258\n",
      "9 0.0004912710622530623\n",
      "10 0.0004393930066966994\n",
      "11 0.0003947341490326503\n",
      "12 0.0003062877291790674\n",
      "13 0.0002095835664638426\n",
      "14 0.00017745078656932782\n",
      "15 0.00015105318367742744\n",
      "16 0.0001169331087822495\n",
      "17 9.01340484456159e-05\n",
      "18 7.437179437454174e-05\n",
      "19 6.462805621523001e-05\n",
      "20 6.0202330747410346e-05\n",
      "21 5.9310339285704505e-05\n",
      "22 5.992841942253896e-05\n",
      "23 6.142286373669376e-05\n",
      "24 6.25134059092358e-05\n",
      "25 6.269937389370624e-05\n",
      "26 6.241379285849071e-05\n",
      "27 6.180549000238282e-05\n",
      "28 6.08798463402852e-05\n",
      "29 5.9644832211774564e-05\n"
     ]
    }
   ],
   "source": [
    "n=Perceptron(X_train,Labels)\n",
    "n.connect(X_train,Labels)\n",
    "n.train(batches=1000,lr=0.2,epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([303, 334, 291, 348, 321, 276, 294, 309, 225, 299]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 88.63333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p3\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0007822106584361049\n",
      "1 0.00040078211128326805\n",
      "2 0.0002809205457411476\n",
      "3 0.00023433983576876728\n",
      "4 0.00024902001142950906\n",
      "5 0.00027669767113873883\n",
      "6 0.000278158105216208\n",
      "7 0.00025786923825464207\n",
      "8 0.00022225494406933206\n",
      "9 0.0001851710858569659\n",
      "10 0.00015601334582149103\n",
      "11 0.0001352027264165157\n",
      "12 0.00012002885545844648\n",
      "13 0.00010641501669875413\n",
      "14 9.193590881772261e-05\n",
      "15 7.778339239270767e-05\n",
      "16 6.537401790104484e-05\n",
      "17 5.500634801370394e-05\n",
      "18 4.648438255240115e-05\n",
      "19 3.951434518975265e-05\n",
      "20 3.381217962481775e-05\n",
      "21 2.9148192736179575e-05\n",
      "22 2.5337501572956882e-05\n",
      "23 2.2227824877671643e-05\n",
      "24 1.968218097568693e-05\n",
      "25 1.7576681551808564e-05\n",
      "26 1.581175712694634e-05\n",
      "27 1.4312954645338953e-05\n",
      "28 1.3025011942487906e-05\n",
      "29 1.1906471220509169e-05\n",
      "30 1.0926208493226439e-05\n",
      "31 1.0061094030893605e-05\n",
      "32 9.293945598488077e-06\n",
      "33 8.611579030517899e-06\n",
      "34 8.00326106779245e-06\n",
      "35 7.45978823145471e-06\n",
      "36 6.973074955670908e-06\n",
      "37 6.535999199519147e-06\n",
      "38 6.142328432168608e-06\n",
      "39 5.786651368010877e-06\n",
      "40 5.464298359796875e-06\n",
      "41 5.17125419281794e-06\n",
      "42 4.904070753415485e-06\n",
      "43 4.659785225067422e-06\n",
      "44 4.435846845828952e-06\n",
      "45 4.2300532584686015e-06\n",
      "46 4.040496260932694e-06\n",
      "47 3.865516134297894e-06\n",
      "48 3.7036634732499355e-06\n",
      "49 3.5536674079272533e-06\n"
     ]
    }
   ],
   "source": [
    "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([304, 324, 300, 323, 307, 283, 290, 334, 248, 287]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 92.5 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p4\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class DoubleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0003265923075598497\n",
      "1 0.00016341079077314275\n",
      "2 8.356097752431184e-05\n",
      "3 5.8029174598968675e-05\n",
      "4 5.0843267588561546e-05\n",
      "5 4.037508028559307e-05\n",
      "6 3.256727908236189e-05\n",
      "7 2.807061543852019e-05\n",
      "8 2.576765910143858e-05\n",
      "9 2.6809080302037223e-05\n",
      "10 3.0171771565228757e-05\n",
      "11 3.269651084447973e-05\n",
      "12 3.275068045474623e-05\n",
      "13 3.0228770870286752e-05\n",
      "14 2.6535788970786563e-05\n",
      "15 2.341606330006755e-05\n",
      "16 2.132304472565122e-05\n",
      "17 1.98125403866199e-05\n",
      "18 1.835963945787632e-05\n",
      "19 1.676442490548928e-05\n"
     ]
    }
   ],
   "source": [
    "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "l2=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([300, 328, 307, 359, 327, 239, 300, 325, 246, 269]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 90.4 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p5\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkActivations():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0002234155291158324\n",
      "1 0.0004760493879802838\n",
      "2 0.0008932903836425521\n",
      "3 0.0005354926710198736\n",
      "4 0.00033677227550146164\n",
      "5 0.00017452658756556534\n",
      "6 0.0001465414010342572\n",
      "7 4.03944826964816e-05\n",
      "8 0.0004707779618681808\n",
      "9 0.00021670996181671133\n",
      "10 1.346765460520391e-05\n",
      "11 4.8920577852112235e-05\n",
      "12 1.7117002512354757e-05\n",
      "13 1.0816918552325357e-05\n",
      "14 5.9445869454898e-05\n",
      "15 1.1271676427299069e-05\n",
      "16 0.0005806703093262877\n",
      "17 2.2141386609807384e-05\n",
      "18 0.0006513266331781528\n",
      "19 2.4979632154188187e-05\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([316, 303, 285, 309, 324, 252, 309, 314, 301, 287]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 90.43333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p6\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkMomentum():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        self.delta_weights=[]\n",
    "        self.delta_bias=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y       \n",
    "    \n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    def backward_pass(y,self,lr,beta=0.9,momentum=False):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            if momentum:\n",
    "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
    "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
    "            else:\n",
    "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,beta,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr,beta,momentum=True)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.933829598571474e-05\n",
      "1 0.00024660438557549844\n",
      "2 0.0005358976964056025\n",
      "3 0.0006313604193986834\n",
      "4 0.00039664294756707635\n",
      "5 0.00044430243002854954\n",
      "6 0.0003537497568280378\n",
      "7 0.00016980686697770457\n",
      "8 0.00020408829218067628\n",
      "9 0.00023075328820914396\n",
      "10 0.0005077852797400178\n",
      "11 0.00012938994302924854\n",
      "12 4.5636338996508255e-05\n",
      "13 0.0003529671218090441\n",
      "14 0.0004693602603072363\n",
      "15 0.00037511196038237537\n",
      "16 0.0003088590383464244\n",
      "17 0.0003415022116476459\n",
      "18 0.00020217897116302846\n",
      "19 0.0004195816743815129\n"
     ]
    }
   ],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([306, 328, 314, 391, 301, 252, 303, 330, 176, 299]),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 90.46666666666667 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
